{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cpKyundWmcHh"
      },
      "source": [
        "# Week 11 - Sequential Decision Making I\n",
        "## Value and Policy Iteration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SPWWYY99mcHj"
      },
      "source": [
        "This jupyter notebook is based on the work made by Massimo Caccia massimo.p.caccia@gmail.com <br>\n",
        "\n",
        "The code was Adapted from: https://github.com/lazyprogrammer/machine_learning_examples/tree/master/rl <br>\n",
        "and then from: https://github.com/omerbsezer/Reinforcement_learning_tutorial_with_demo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W-5UimCjmcHj"
      },
      "source": [
        "## 0. Preliminaries\n",
        "\n",
        "Before we jump into the value and policy iteration excercies, we will test your comprehension of a Markov Decision Process (MDP). <br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oiYpQMG6mcHk"
      },
      "source": [
        "### 0.1 Tic-Tac-Toe\n",
        "\n",
        "Let's take a simple example: Tic-Tac-Toe (also known as Tic-tac-toe, noughts and crosses, or Xs and Os). Definition: it is a paper-and-pencil game for two players, X and O, who take turns marking the spaces in a 3Ã—3 grid. The player who succeeds in placing three of their marks in a horizontal, vertical, or diagonal row is the winner."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lg3IC1qxmcHk",
        "outputId": "e2b72a4f-8255-4205-cdd9-168380d83bfe"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Image\n",
        "from IPython.core.display import HTML\n",
        "Image(url= \"https://bjc.edc.org/bjc-r/img/3-lists/TTT1_img/Three%20States%20of%20TTT.png\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2_YOYZGnmcHl"
      },
      "source": [
        "**Question:** Imagine you were trying to build an agent for this game. Let's try to describe how we would model it. Specifically, what are the states, actions, transition function and rewards?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hqN8JSs2mcHl"
      },
      "source": [
        "**Answer:**<br>\n",
        "The **state space** is a 3x3 Matrix or a vector of length 9 that indicates if a particular spot is: a) empty, b) taken by X or c) taken by O. <br>\n",
        "\n",
        "The **actions** are on which of the 9 spot you can play (so there is 9 possible actions). Note that as the game evolves, some actions will become unavailable. <br>\n",
        "\n",
        "An example of a **reward function** could return +1 if you win, -1 if you lose, and 0 for a draw.\n",
        "\n",
        "The **transition function** is dictated by your opponent's strategy. <br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rnLLDuLSmcHo"
      },
      "source": [
        "## 1. Value Iteration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2WcVOJjOmcHo"
      },
      "source": [
        "The exercises will test your capacity to **complete the value iteration algorithm**.\n",
        "\n",
        "You can find details about the algorithm at slide 62 and 63 of the [slide](https://blackboard.lincoln.ac.uk/bbcswebdav/pid-9042712-dt-content-rid-20004891_2/xid-20004891_2) deck. <br>\n",
        "\n",
        "The algorithm will be tested on a simple Gridworld similar to the one presented in classroom."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ZvuV9qNmcHp"
      },
      "source": [
        "### 1.1 Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QFs3ZLKCmcHp",
        "outputId": "f316823f-56dc-4d35-e4b2-4bde32c543eb"
      },
      "outputs": [],
      "source": [
        "#imports\n",
        "\n",
        "!wget -nc https://raw.githubusercontent.com/lcharlin/80-629/master/week12-MDPs/gridWorldGame.py\n",
        "\n",
        "import numpy as np\n",
        "from gridWorldGame import standard_grid, negative_grid, print_values, print_policy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ibmF_JKSmcHq"
      },
      "source": [
        "Let's set some variables. <br>\n",
        "`SMALL_ENOUGH` is a threshold we will utilize to determine the convergence of value iteration<br>\n",
        "`GAMMA` is the discount factor denoted $\\gamma$ in the slides<br>\n",
        "`ALL_POSSIBLE_ACTIONS` are the actions you can take in the GridWold, as in slide 12. In this simple grid world, we will have four actions: Up, Down, Right, Left. <br>\n",
        "`NOISE_PROB` defines how stochastic the environement is. It is the probability that the environment takes you where a random action would."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bO8JkPpamcHq"
      },
      "outputs": [],
      "source": [
        "SMALL_ENOUGH = 1e-3 # threshold to declare convergence\n",
        "GAMMA = 0.9         # discount factor\n",
        "ALL_POSSIBLE_ACTIONS = ('U', 'D', 'L', 'R') # Up, Down, Left, Right\n",
        "NOISE_PROB = 0.1    # Probability of the agent not reaching it's intended goal after an action"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5wY3IcqRmcHq"
      },
      "source": [
        "Now we will set up a the Gridworld. <br>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JxlzMN1fmcHq",
        "outputId": "51901558-2e6d-4449-af38-de4e03ba98ef"
      },
      "outputs": [],
      "source": [
        "grid = standard_grid(noise_prob=NOISE_PROB)\n",
        "print(\"rewards:\")\n",
        "print_values(grid.rewards, grid)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uL4zrAIPmcHr"
      },
      "source": [
        "There are three absorbing states: (0,3),(1,3), and (1,1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eqIg1uaJmcHr"
      },
      "source": [
        "Next, we will define a random inital policy $\\pi$. <br>\n",
        "Remember that a policy maps states to actions $\\pi : S \\rightarrow A$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "727FaS2wmcHr",
        "outputId": "56856c64-a6c7-4fd5-d268-e7e6f819b69e"
      },
      "outputs": [],
      "source": [
        "policy = {}\n",
        "for s in grid.actions.keys():\n",
        "    policy[s] = np.random.choice(ALL_POSSIBLE_ACTIONS)\n",
        "\n",
        "# initial policy\n",
        "print(\"initial policy:\")\n",
        "print_policy(policy, grid)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AZLwRuD0mcHr"
      },
      "source": [
        "Note that there is no policy in the absorbing/terminal states (hence the Not Available \"N/A\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dA_gusK3mcHr"
      },
      "source": [
        "Next, we will randomly initialize the value function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "epXZ9KRcmcHr",
        "outputId": "a1aabd9a-c518-4814-f605-95aa9c5bb6dd"
      },
      "outputs": [],
      "source": [
        "np.random.seed(1234) # make sure this is reproducable\n",
        "\n",
        "V = {}\n",
        "states = grid.all_states()\n",
        "for s in states:\n",
        "    # V[s] = 0\n",
        "    if s in grid.actions:\n",
        "        V[s] = np.random.random()\n",
        "    else:\n",
        "        # terminal state\n",
        "        V[s] = 0\n",
        "\n",
        "# initial value for all states in grid\n",
        "print_values(V, grid)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8bHauBqmmcHr"
      },
      "source": [
        "Note that we set to Null the values of the terminal states. <br>\n",
        "For the print_values() function to compile, we set them to 0."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xw5vpC2wmcHr"
      },
      "source": [
        "### 1.2 Value iteration algorithms - code completion\n",
        "\n",
        "You will now have to complete the Value iteration algorithm. <br>\n",
        "Remember that, for each iteration, each state s need to have to be update with the formula:\n",
        "\n",
        "$$\n",
        "% V(s) = \\underset{a}{max}\\big\\{ \\sum_{s'}  p(s'|s,a)(r + \\gamma*V(s') \\big\\}\n",
        "V(s) = r + \\gamma * \\underset{a \\in A}{max}\\big\\{ \\sum_{s'}  p(s'|s,a) V(s') \\big\\}\n",
        "$$\n",
        "Note that in the current gridWorld, p(s'|s,a) is deterministic. <br>\n",
        "Also, remember that in value iteration, the policy is implicit. <br> Thus, you don't need to update it at every iteration. <br>\n",
        "Run the algorithm until convergence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BhZAU9KWmcHs",
        "outputId": "27cdd9ad-86c7-4120-df6a-3b194df9331e",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "iteration=0\n",
        "while True:\n",
        "    print(\"VI iteration %d: \" % iteration)\n",
        "    print_values(V, grid)\n",
        "    print(\"\\n\\n\")\n",
        "\n",
        "    biggest_change = 0\n",
        "    for s in states:\n",
        "        old_v = V[s]\n",
        "\n",
        "        # V(s) only has value if it's not a terminal state\n",
        "        if s in policy:\n",
        "            new_v = float('-inf')\n",
        "\n",
        "            # for each action\n",
        "            for a in ALL_POSSIBLE_ACTIONS:\n",
        "                grid.set_state(s)\n",
        "                r = grid.move(a)\n",
        "                sprime = grid.current_state()\n",
        "                #  TODO 1 - compute this V[s] = r + gamma * max[a]{ sum[s'] { p(s',r|s,a)V[s'] for the current state s and action a\n",
        "                # and keep track of the new utility if it's better than the old one\n",
        "                # *** YOUR CODE HERE ***\n",
        "\n",
        "                \n",
        "            V[s] = new_v\n",
        "            biggest_change = max(biggest_change, np.abs(old_v - V[s]))\n",
        "\n",
        "    print('\\t biggest change is: %f \\n\\n' % biggest_change)\n",
        "    # TODO 2 - terminate teh algorithm at convergence\n",
        "    # *** YOUR CODE HERE ***\n",
        "\n",
        "\n",
        "    iteration+=1\n",
        "print_values(V, grid)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RO2GFjTjmcHs"
      },
      "source": [
        "Now that the value function is optimized, we use it to find the optimal policy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TVnKifQomcHs"
      },
      "outputs": [],
      "source": [
        "deterministic_grid = standard_grid(noise_prob=0.)\n",
        "\n",
        "for s in policy.keys():\n",
        "    best_a = None\n",
        "    best_value = float('-inf')\n",
        "    # loop through all possible actions to find the best current action\n",
        "    for a in ALL_POSSIBLE_ACTIONS:\n",
        "        deterministic_grid.set_state(s)\n",
        "        r = deterministic_grid.move(a)\n",
        "        # TODO 3 : compute the utility in the new current state\n",
        "        # *** YOUR CODE HERE ***\n",
        "\n",
        "        if v > best_value:\n",
        "            # TODO 4: if the new utility is better than the old one, we select the associated action\n",
        "            # *** YOUR CODE HERE ***\n",
        "            \n",
        "    policy[s] = best_a"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cZLHHs6AmcHs"
      },
      "source": [
        "Now print your policy and make sure it leads to the upper-right corner which is the termnial state returning the most rewards."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NjnvBntKmcHt",
        "outputId": "113d14bd-f335-402a-bd53-00ff7d16736f"
      },
      "outputs": [],
      "source": [
        "print(\"values:\")\n",
        "print_values(V, grid)\n",
        "print(\"\\npolicy:\")\n",
        "print_policy(policy, grid)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E23Vs6bxmcHt"
      },
      "source": [
        "## 2. Policy Iteration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uXqsFkUWmcHt"
      },
      "source": [
        "You will be tested on your capacity to **complete the poliy iteration algorithm**. <br>\n",
        "You can find details about the algorithm at slide 78 of the slide deck. <br>\n",
        "The algorithm will be tested on a simple Gridworld similar to the one presented at slide 12. <br>\n",
        "This Gridworld is however simpler because the MDP is deterministic. <br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TobT0t5UmcHt"
      },
      "source": [
        "First we will define a random inital policy. <br>\n",
        "Remember that a policy maps states to actions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rsx6Sh4PmcHt",
        "outputId": "9c7fd8ac-2753-4cbe-c318-d6a4936bb369"
      },
      "outputs": [],
      "source": [
        "policy = {}\n",
        "for s in grid.actions.keys():\n",
        "    policy[s] = np.random.choice(ALL_POSSIBLE_ACTIONS)\n",
        "\n",
        "# initial policy\n",
        "print(\"initial policy:\")\n",
        "print_policy(policy, grid)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ep4vpbMbmcHu"
      },
      "source": [
        "Next, we will randomly initialize the value function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uTFpVni9mcHu",
        "outputId": "74d2af17-becd-4001-c2be-bd2e18e1f0bb"
      },
      "outputs": [],
      "source": [
        "np.random.seed(1234)\n",
        "\n",
        "# initialize V(s) - value function\n",
        "V = {}\n",
        "states = grid.all_states()\n",
        "for s in states:\n",
        "    if s in grid.actions:\n",
        "        V[s] = np.random.random()\n",
        "    else:\n",
        "        # terminal state\n",
        "        V[s] = 0\n",
        "\n",
        "# initial value for all states in grid\n",
        "print_values(V, grid)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HBW2Ojf_mcHu"
      },
      "source": [
        "Note that we set to Null the values of the terminal states. <br>\n",
        "For the print_values() function to compile, we set them to 0."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CectfJvymcHu"
      },
      "source": [
        "### 2.2 Policy iteration - code completion\n",
        "\n",
        "You will now have to complete the Policy iteration algorithm. <br>\n",
        "Remember that the algorithm works in two phases. <br>\n",
        "First, in the *policy evaluation* phase, the value function is update with the formula:\n",
        "\n",
        "$$\n",
        "% V^\\pi(s) =  \\sum_{s'}  p(s'|s,\\pi(s))(r + \\gamma*V^\\pi(s')\n",
        "V^\\pi(s) = r + \\gamma * \\sum_{s'} p(s'|s,\\pi(s))*V^\\pi(s')\n",
        "$$\n",
        "This part of the algorithm is already coded for you. <br>\n",
        "\n",
        "Second, in the *policy improvement* step, the policy is updated with the formula:\n",
        "\n",
        "$$\n",
        "% \n",
        "\\pi'(s) = \\underset{a \\in A}{arg max}\\big\\{ \\sum_{s'}  p(s'|s,a)*V^\\pi(s') \\big\\}\n",
        "$$\n",
        "\n",
        "This is the part of code you will have to complete. <br>\n",
        "\n",
        "Note that in the current gridWorld, p(s'|s,a) is deterministic. <br>\n",
        "Run the algorithm until convergence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F5xQus34mcHv",
        "outputId": "0ab49d2a-d33d-4a61-953d-6dc565e61662",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "iteration=0\n",
        "# repeat until the policy does not change\n",
        "while True:\n",
        "    print(\"values (iteration %d)\" % iteration)\n",
        "    print_values(V, grid)\n",
        "    print(\"policy (iteration %d)\" % iteration)\n",
        "    print_policy(policy, grid)\n",
        "    print('\\n\\n')\n",
        "\n",
        "    # 1. policy evaluation step\n",
        "    # this implementation does multiple policy-evaluation steps\n",
        "    # this is different than in the algorithm from the slides\n",
        "    # which does a single one.\n",
        "    while True:\n",
        "        biggest_change = 0\n",
        "        for s in states:\n",
        "            old_v = V[s]\n",
        "\n",
        "            # V(s) only has value if it's not a terminal state\n",
        "            if s in policy:\n",
        "                a = policy[s]\n",
        "                grid.set_state(s)\n",
        "                r = grid.move(a) # reward\n",
        "                sprime = grid.current_state() # s'\n",
        "                V[s] = r + GAMMA * V[sprime]\n",
        "            biggest_change = max(biggest_change, np.abs(old_v - V[s]))\n",
        "        if biggest_change < SMALL_ENOUGH:\n",
        "            break\n",
        "\n",
        "    #2. policy improvement step\n",
        "    is_policy_converged = True\n",
        "    for s in states:\n",
        "        if s in policy:\n",
        "            old_a = policy[s]\n",
        "            new_a = None\n",
        "            best_value = float('-inf')\n",
        "            # loop through all possible actions to find the best current action\n",
        "            for a in ALL_POSSIBLE_ACTIONS:\n",
        "                grid.set_state(s)\n",
        "                r = grid.move(a)\n",
        "                sprime = grid.current_state()\n",
        "                # TODO 5: compute the new utility value and, if better update the picked action \n",
        "                # *** YOUR CODE HERE ***\n",
        "\n",
        "                if v > best_value:\n",
        "                    # *** YOUR CODE HERE ***\n",
        "                    \n",
        "            if new_a is None:\n",
        "                print('problem')\n",
        "            policy[s] = new_a\n",
        "            if new_a != old_a:\n",
        "                is_policy_converged = False\n",
        "\n",
        "    if is_policy_converged:\n",
        "        break\n",
        "    iteration+=1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NvdALk81mcHv"
      },
      "source": [
        "Now print your policy and make sure it leads to the upper-right corner which is the termnial state returning the most rewards."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sqDBKbpwmcHv",
        "outputId": "27e1e423-6450-46d6-b288-09bd6a301ab6"
      },
      "outputs": [],
      "source": [
        "print(\"final values:\")\n",
        "print_values(V, grid)\n",
        "print(\"final policy:\")\n",
        "print_policy(policy, grid)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m9snggvdmcHw"
      },
      "source": [
        "#"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
